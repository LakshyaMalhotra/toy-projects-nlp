{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "This is a code along of the excellent tutorial series by [bentrevett](https://github.com/bentrevett/pytorch-seq2seq). It is mostly for learning and self assessment.\n",
    "\n",
    "The first part of this tutorial series is basically the implementation of [Sequence to sequence learning with neural networks](https://arxiv.org/abs/1409.3215) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The most common seq-to-seq models use an encoder-decoder network. Both of these use a recurrent neural networks. Encoder takes the source sentence as input and encode it into a single vector called _context vector_. This vector is then decoded by the decoder to generate the output sequence one token at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "We will be using PyTorch and torchtext for the network architecture and pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "SEED = 23\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create tokenizers for English and German languages. spaCy has model for different languages which we can use to access the tokenizers.\n",
    "\n",
    "Once downloaded, the models can be easily loaded by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create tokenizer functions. These will take in the sentence and return the sentence as a list of tokens.\n",
    "\n",
    "Quoting the paper: *While the LSTM is capable of solving problems with long term dependencies, we discovered that\n",
    "the LSTM learns much better when the source sentences are reversed (the target sentences are NOT reversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decode*d translations increased from 25.9 to 30.6.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text from a string and create a list of tokens after reversing it.\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text from a string and create a list of tokens.\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire pre-processing pipeline can be easily implemented using torchtext. Check out the constructor arguments [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L61)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lak91/.virtualenvs/nlp/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "source = Field(tokenize=tokenize_de,\n",
    "               init_token=\"<sos>\",\n",
    "               eos_token=\"<eos>\",\n",
    "               lower=True, \n",
    "               include_lengths=True, \n",
    "               batch_first=True)\n",
    "target = Field(tokenize=tokenize_en,\n",
    "               init_token=\"<sos>\", \n",
    "               eos_token=\"<eos>\", \n",
    "               lower=True, \n",
    "               include_lengths=True, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's the time to download the data and create train, validation and test data. The dataset we are using is [Multi30k](https://github.com/multi30k/dataset), it contains about 30,000 parallel English, French and German sentences. It is also available through torchtext. \n",
    "\n",
    "`exts` specifies which languages to use as the source and target (source goes first) and `fields` specifies which field to use for the source and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 500kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 205kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 22.3MB/s]\n",
      "/home/lak91/.virtualenvs/nlp/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), \n",
    "                                                    fields=(source, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of test examples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check the data\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of test examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei']\n",
      "trg: ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "for key, words in vars(train_data.examples[0]).items():\n",
    "    print(key+\":\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the period in the german sentence is at index 0, this means the input sentences are correctly reversed.\n",
    "\n",
    "Once we have data we can create vocabularies for the source and target languages. Torchtext provides some utilities for that as well. We can check different options out [here](https://torchtext.readthedocs.io/en/latest/data.html#field).\n",
    "\n",
    "We just include tokens which are repeated at least 2 times in the train data. Any token used only once is converted to <UNK> (unknown) token.\n",
    "\n",
    "For building vocabulary, we just use train set to avoid any data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.build_vocab(train_data, min_freq=2)\n",
    "target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in german vocabulary: 7854\n",
      "Number of unique tokens in english vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "# Check out the vocabularies\n",
    "print(f\"Number of unique tokens in german vocabulary: {len(source.vocab)}\")\n",
    "print(f\"Number of unique tokens in english vocabulary: {len(target.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "GeForce GTX 1070\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those of you are familiar with PyTorch API know that the next step in the pipeline is to create dataloaders which create data batches. In torchtext, this can be done using iterators.\n",
    "\n",
    "When we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences. Luckily, `TorchText` iterators handle this for us!\n",
    "\n",
    "We use a `BucketIterator` instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lak91/.virtualenvs/nlp/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=batch_size, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2seq Model\n",
    "\n",
    "The model has three components: Encoder, Decoder and seq2seq model which encapsulates both encoder and decoder.\n",
    "\n",
    "### Encoder\n",
    "Encoder is just a recurrent neural network. For our case, we are going to start with a GRU and then try a LSTM. We are also going to try different variants of RNN like bidirectional, different number of layers to increase the robustness and to provide context from both previous and subsequent time steps.\n",
    "\n",
    "We implement encoder by creating a `Encoder` class. It takes the following arguments:\n",
    "- `input_size`: number of rows in the embedding matrix. It is nothing but the vocabulary for the `source`\n",
    "- `embed_dim`: embedding dimension which gives number of components for each word vector in the embedding space\n",
    "- `hidden_size`: size of the hidden state(as well as the cell states in the case of LSTMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(input_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embed(src))\n",
    "        # embedded shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # output shape: [batch_size, seq_len, hidden_size*n_directions]\n",
    "        # hidden shape: [n_layers*n_directions, batch_size, hidden_size];\n",
    "        # batch size is in `dim=1` of the hidden size even after setting `batch_first`=True\n",
    "\n",
    "        # For encoder, we are not interested in the output so skipping returning it\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "Like encoder, our decoder is also a RNN. The key feature of the decoder is that the hidden state from the encoder acts as the \"context-vector\" and will be treated as the hidden state for the first time step of the decoder. Unlike encoder, we start by feeding in the `<SOS>` token to the decoder and then feed the target sentence (or prediction from the current time step) token by token.  We will employ a technique called \"teacher-forcing\" in which we sample the decoder input from the current prediction or the target sentence (ground-truth) based on some probability. This is only used during training and it further ensures the robustness of the model.\n",
    "\n",
    "Later we will also implement Attention model in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_dim, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embed = nn.Embedding(output_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input shape: [batch_size]\n",
    "        input = input.unsqueeze(-1)\n",
    "        # input shape: [batch_size, 1]\n",
    "\n",
    "        embedded = self.dropout(self.embed(input))\n",
    "        # embedded shape: [batch_size, 1, embed_dim]\n",
    "\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output shape: [batch_size, seq_len, n_directions*hidden_size]\n",
    "        # hidden shape: [n_directions*n_layers, batch_size, hidden_size]\n",
    "        # seq_len is always 1 in the decoder\n",
    "\n",
    "        output = self.linear(output.squeeze(1))\n",
    "        # output shape: [batch_size, output_size]\n",
    "\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        # output shape: [batch_size, output_size]\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq\n",
    "Finally, we will encapsulate both encoder and decoder into `Seq2Seq` model class. It will be our black box where we receive input/target, generate context vectors from encoders and produce predicted output rom the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device=torch.device(\"cpu\")):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor, \n",
    "                teacher_forcing_ratio=0.5):\n",
    "        # input_tensor shape: [batch_size, seq_len]\n",
    "        # target_tensor shape: [batch_size, seq_len]\n",
    "        batch_size = target_tensor.shape[0]\n",
    "        target_length = target_tensor.shape[-1]\n",
    "\n",
    "        output_size = self.decoder.output_size \n",
    "        # size of the target vocabulary\n",
    "\n",
    "        outputs = torch.zeros(\n",
    "            batch_size, target_length, output_size).to(self.device)\n",
    "        \n",
    "        # we don't provide any hidden state to the encoder since pytorch\n",
    "        # by default initializes it to zeros if not provided\n",
    "        encoder_hidden = self.encoder(input_tensor)\n",
    "\n",
    "        # first input to the decoder is just <SOS> token\n",
    "        input = target_tensor[:, 0]\n",
    "\n",
    "        # decoder hidden at the first time step is just encoder hidden (context vector)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # iterate through the length of the target tensor starting from the second token\n",
    "        for ti in range(1, target_length):\n",
    "            # forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(input, decoder_hidden)\n",
    "\n",
    "            # place decoder output for the given time step into outputs tensor\n",
    "            outputs[:, ti, :] = decoder_output\n",
    "\n",
    "            # decide if we want to use teacher forcing \n",
    "            teacher_forcing = (\n",
    "                True if random.random() < teacher_forcing_ratio else False\n",
    "            )\n",
    "\n",
    "            # get the token with highest probability from the output\n",
    "            top1 = decoder_output.argmax(1)\n",
    "\n",
    "            # if teacher forcing, use actual next token as the decoder input\n",
    "            # else, use the prediction\n",
    "            input = (\n",
    "                target_tensor[:, ti]\n",
    "                if teacher_forcing\n",
    "                else top1\n",
    "            )\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "With all the model components defined, it's time for training the model now. First, we need to instantiate it by defining few parameters like `input_size`, `output_size`, `hidden_size`, and `embedding_dim` for both encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(7854, 500)\n",
       "    (rnn): GRU(500, 1024, batch_first=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(5893, 500)\n",
       "    (rnn): GRU(500, 1024, batch_first=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (linear): Linear(in_features=1024, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(source.vocab)\n",
    "output_size = len(target.vocab)\n",
    "\n",
    "enc_embed_dim = 500\n",
    "enc_hidden_size = 1024\n",
    "\n",
    "dec_embed_dim = 500\n",
    "dec_hidden_size = 1024\n",
    "\n",
    "encoder = Encoder(input_size, enc_embed_dim, enc_hidden_size)\n",
    "decoder = Decoder(output_size, dec_embed_dim, dec_hidden_size)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device=device).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a function which will tell us the number of the trainable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has a total of 22,289,569 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has a total of {count_params(model):,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last piece of objects needed before we can actually write training loop are the loss functions and optimizers. Also we also don't want to calculate the loss on the `<pad>` token, this can be done with `ignore_index` argument in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the string representation of first 4 target tokens\n",
    "target.vocab.itos[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = target.vocab.stoi[target.pad_token]\n",
    "unk_idx = target.vocab.stoi[target.unk_token]\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=pad_idx)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we are in the position to start the training. We will write two separate functions: one for training and other for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        input_tensor = batch.src\n",
    "        target_tensor = batch.trg\n",
    "\n",
    "        # not using the batch lengths for each \n",
    "        # batch of input and output tensors\n",
    "        input_tensor = input_tensor[0]\n",
    "        target_tensor = target_tensor[0]\n",
    "\n",
    "        # zeroing out any stray grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(input_tensor, target_tensor)\n",
    "        # target_tensor = [batch_size, seq_len]\n",
    "        # output = [batch_size, seq_len, output_size]\n",
    "\n",
    "        output_size = output.size(-1)\n",
    "\n",
    "        # ignoring the <SOS> token\n",
    "        output = output[1:].view(-1, output_size)\n",
    "        target_tensor = target_tensor[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, target_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_tensor = batch.src\n",
    "            target_tensor = batch.trg\n",
    "\n",
    "            input_tensor = input_tensor[0]\n",
    "            target_tensor = target_tensor[0]\n",
    "            \n",
    "            output = model(input_tensor, target_tensor, 0)\n",
    "            # target_tensor = [batch_size, seq_len]\n",
    "            # output = [batch_size, seq_len, output_size]\n",
    "\n",
    "            output_size = output.size(-1)\n",
    "\n",
    "            # ignoring the <SOS> token\n",
    "            output = output[1:].view(-1, output_size)\n",
    "            target_tensor = target_tensor[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, target_tensor)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 31s\n",
      "\tTrain Loss: 3.694 | Train PPL:  40.225\n",
      "\t Val. Loss: 3.693 |  Val. PPL:  40.160\n",
      "Epoch: 02 | Time: 1m 32s\n",
      "\tTrain Loss: 2.913 | Train PPL:  18.419\n",
      "\t Val. Loss: 3.529 |  Val. PPL:  34.090\n",
      "Epoch: 03 | Time: 1m 32s\n",
      "\tTrain Loss: 2.539 | Train PPL:  12.663\n",
      "\t Val. Loss: 3.399 |  Val. PPL:  29.938\n",
      "Epoch: 04 | Time: 1m 32s\n",
      "\tTrain Loss: 2.266 | Train PPL:   9.643\n",
      "\t Val. Loss: 3.475 |  Val. PPL:  32.310\n",
      "Epoch: 05 | Time: 1m 32s\n",
      "\tTrain Loss: 2.073 | Train PPL:   7.946\n",
      "\t Val. Loss: 3.485 |  Val. PPL:  32.625\n",
      "Epoch: 06 | Time: 1m 31s\n",
      "\tTrain Loss: 1.926 | Train PPL:   6.860\n",
      "\t Val. Loss: 3.503 |  Val. PPL:  33.217\n",
      "Epoch: 07 | Time: 1m 31s\n",
      "\tTrain Loss: 1.814 | Train PPL:   6.132\n",
      "\t Val. Loss: 3.639 |  Val. PPL:  38.044\n",
      "Epoch: 08 | Time: 1m 30s\n",
      "\tTrain Loss: 1.754 | Train PPL:   5.776\n",
      "\t Val. Loss: 3.657 |  Val. PPL:  38.760\n",
      "Epoch: 09 | Time: 1m 31s\n",
      "\tTrain Loss: 1.695 | Train PPL:   5.447\n",
      "\t Val. Loss: 3.749 |  Val. PPL:  42.475\n",
      "Epoch: 10 | Time: 1m 31s\n",
      "\tTrain Loss: 1.659 | Train PPL:   5.254\n",
      "\t Val. Loss: 3.773 |  Val. PPL:  43.508\n",
      "Epoch: 11 | Time: 1m 31s\n",
      "\tTrain Loss: 1.626 | Train PPL:   5.085\n",
      "\t Val. Loss: 3.834 |  Val. PPL:  46.225\n",
      "Epoch: 12 | Time: 1m 31s\n",
      "\tTrain Loss: 1.616 | Train PPL:   5.031\n",
      "\t Val. Loss: 3.884 |  Val. PPL:  48.601\n",
      "Epoch: 13 | Time: 1m 31s\n",
      "\tTrain Loss: 1.591 | Train PPL:   4.909\n",
      "\t Val. Loss: 3.896 |  Val. PPL:  49.182\n",
      "Epoch: 14 | Time: 1m 30s\n",
      "\tTrain Loss: 1.570 | Train PPL:   4.806\n",
      "\t Val. Loss: 3.970 |  Val. PPL:  52.995\n",
      "Epoch: 15 | Time: 1m 31s\n",
      "\tTrain Loss: 1.560 | Train PPL:   4.759\n",
      "\t Val. Loss: 4.023 |  Val. PPL:  55.846\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, \n",
    "                       device, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'plain-rnn-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
